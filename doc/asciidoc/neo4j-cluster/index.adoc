
[#neo4j_causal_cluster]
== Using with Neo4j Causal Cluster

ifdef::env-docs[]
[abstract]
--
This chapter describes considerations around using Neo4j Streams with Neo4j Enterprise Causal Cluster.
--
endif::env-docs[]

=== Overview

link:https://neo4j.com/docs/operations-manual/current/clustering/[Neo4j Clustering] is a feature available in
Enterprise Edition which allows high availability of the database through having multiple database members.

Neo4j Enterprise uses a link:https://neo4j.com/docs/operations-manual/current/clustering/introduction/#causal-clustering-introduction-operational[LEADER/FOLLOWER]
operational view, where writes are always processed by the leader, while reads can be serviced by either followers,
or optionally be read replicas, which maintain a copy of the database and serve to scale out read operations
horizontally.


[#cluster_kafka_connect]
=== Kafka Connect

When using Neo4j Streams in this method with a Neo4j cluster, the most important consideration is to use
a routing driver.  Generally, these are identified by a URI with `bolt+routing://` as the scheme instead of
just `bolt://` as the scheme.

If your Neo4j cluster is located at `graph.mycompany.com:7687`, simply configure the Kafka Connect worker with

----
neo4j.server.uri=bolt+routing://graph.mycompany.com:7687
----

The use of the `bolt+routing` driver will mean that the Neo4j Driver itself will handle connecting to
the correct cluster member, and managing changes to the cluster membership over time.

For further information on routing drivers, see the link:https://neo4j.com/docs/driver-manual/current/[Neo4j Driver Manual].

=== Neo4j Plugin

When using Neo4j Streams as a plugin together with a cluster, there are several things to keep in mind:

* The plugin must be present in the plugins directory of _all cluster members_, and not just one.
* The configuration settings must be present in:
   - *all neo4j.conf files* for versions prior of `3.5.13`;
   - *all streams.conf files* for versions since `3.5.13`.

Through the course of the cluster lifecycle, the leader may change; for this reason the plugin must be everywhere,
and not just on the leader.

The plugin detects the leader, and will not attempt to perform a write (i.e. in the case of the consumer)
on a follower where it would fail.  The plugin checks cluster toplogy as needed.

Additionally for CDC, a consideration to keep in mind is that as of Neo4j 3.5, committed transactions are only
published on the leader as well.  In practical terms, this means that as new data is committed to Neo4j, it is
the leader that will be publishing that data back out to Kafka, if you have the producer configured.

The neo4j streams utility procedures, in particular `CALL streams.publish`, can work on any cluster member, or
read replica. `CALL streams.consume` may also be used on any cluster member, however it is important to keep in
mind that due to the way clustering in Neo4j works, using `streams.consume` together with write operations will
not work on a cluster follower or read replica, as only the leader can process writes.

==== Update the configuration via procedures (since version 3.5.13)

N.b. this mode implies that `streams.procedures.enabled` is set to `true`

In a causal cluster composed by 3 `CORE` members you can use Kafka in order to update
the configuration to all the nodes by executing a single query from the `LEADER`.

In order to do that we leverage the `apoc.periodic.repeat` procedure.

You need to "install" the following query in all core nodes

[source,cypher]
----
CALL apoc.periodic.repeat("streams.update.config", "
  CALL dbms.cluster.role() YIELD role
  WHERE role <> 'LEADER'
  CALL streams.consume('neo4j-kafka-config', {`kafka.group.id`: '<one-id-per-core-instance>', timeout: 2000}) YIELD event
  CALL streams.configuration.set(event.data.payload) YIELD name, value
  RETURN name, value
", 5)
----

*N.b.* is important to define one `kafka.group.id` per CORE instance in
order to allow them to consume the same topic independently.

Then every time that you need to update the configuration you can use
the following template that must be executed from the `LEADER`:

[source,cypher]
----
WITH {`streams.sink.topic.cypher.mytopic`: "CREATE (p:Person{name: event.name, surname: event.surname})"} AS event
CALL streams.publish('neo4j-kafka-config', event)
CALL streams.configuration.set(event) YIELD name, value
RETURN name, value
----

(The `event` variable map is the map of the properties that you want to change)

automatically the configuration gets updated inside every `CORE` node.

=== Remote Clients

Sometimes there will be remote applications that talk to Neo4j via official drivers, that want to use
streams functionality. Best practices in these cases are:

* Always use a `bolt+routing://` driver URI when communicating with the cluster in the client application.
* Use link:https://neo4j.com/docs/driver-manual/current/sessions-transactions/#driver-transactions[Explicit Write Transactions] in
your client application when using procedure calls such as `CALL streams.consume` to ensure that the routing
driver routes the query to the leader.